# --- Th∆∞ vi·ªán ---
import os
from PIL import Image, ImageOps
from tqdm import tqdm 
import torch
import numpy as np
from transformers import AutoImageProcessor, AutoModel
import json
import time
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler
import umap

# G·ª° b·ªè gi·ªõi h·∫°n pixel cho ·∫£nh ƒë·ªô ph√¢n gi·∫£i l·ªõn
Image.MAX_IMAGE_PIXELS = None  


# --- C·∫•u h√¨nh chung ---
INPUT_FOLDER = "public/test" # Folder input
THUMB_FOLDER = "public/thumbnail" # Folder output
THUMB_SIZE = 256 # K√≠ch th∆∞·ªõc c·∫°nh l·ªõn nh·∫•t
THUMB_QUALITY = 80 # Gi·ªØ l·∫°i ch·∫•t l∆∞·ª£ng 80%

# H√†m t·∫°o thumbnail
def create_thumbnails(input_dir, output_dir):
    """
    H√†m t·∫°o ·∫£nh thu nh·ªè t·ªëi ∆∞u cho d·ª± √°n 
    S·ª≠ d·ª•ng thu·∫≠t to√°n Lanczos ƒë·ªÉ ·∫£nh thu nh·ªè nh∆∞ng v·∫´n gi·ªØ ƒë·ªô chi ti·∫øt cao

    Args:
        input_dir (_type_): _description_
        output_dir (_type_): _description_
    """
    # Ki·ªÉm tra v√† t·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Danh s√°ch file ·∫£nh g·ªëc h·ª£p l·ªá
    valid_exts = ('.jpg', '.jpeg', '.png', '.webp', '.bmp', '.tiff')
    files = [f for f in os.listdir(input_dir) if f.lower().endswith(valid_exts)]
    
    count = 0
    for filename in tqdm(files, desc="üî® Creating Thumbnails", unit="img"):
        # ƒê∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi c·ªßa file ·∫£nh
        src_path = os.path.join(input_dir, filename)
        # L·∫•y t√™n ·∫£nh + ƒëu√¥i file m·ªõi
        dst_name = os.path.splitext(filename)[0] + ".webp"
        # ƒê∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi m·ªõi = ƒë∆∞·ªùng d·∫´n output + filename.webp
        dst_path = os.path.join(output_dir, dst_name)

        # N·∫øu ·∫£nh thumbnail ƒë√£ c√≥ r·ªìi th√¨ b·ªè qua lu√¥n, kh√¥ng m·ªü ·∫£nh g·ªëc ra n·ªØa
        if os.path.exists(dst_path):
            continue

        try:
            # M·ªü ·∫£nh g·ªëc
            with Image.open(src_path) as img:
                # Convert qua RGB: V√¨ ·∫£nh PNG c√≥ k√™nh Alpha (trong su·ªët)
                img = img.convert("RGB")

                # Tr√°nh xoay ·∫£nh
                img = ImageOps.exif_transpose(img)
                
                # Gi·ªØ nguy√™n t·ªâ l·ªá, ch·ªâ thu nh·ªè sao cho v·ª´a khung THUMB_SIZE x THUMB_SIZE
                img.thumbnail((THUMB_SIZE, THUMB_SIZE), Image.Resampling.LANCZOS)

                # L∆∞u ·∫£nh
                img.save(dst_path, "WEBP", quality=THUMB_QUALITY)
                count += 1
        except Exception as e:
            print(f"‚ùå L·ªói file {filename}: {e}")

class FeatureExtractor:
    def __init__(self,model_name='facebook/dinov2-base'):
        """
        Kh·ªüi t·∫°o m√¥ h√¨nh AI.
        Load model m·ªôt l·∫ßn duy nh·∫•t ƒë·ªÉ d√πng ƒëi d√πng l·∫°i.
        """      
        # ∆Øu ti√™n: CUDA (NVIDIA) -> MPS (Mac Silicon) -> CPU
        if torch.cuda.is_available():
            self.device = "cuda"
            print(f"üß† ƒêang t·∫£i model: {model_name} l√™n üöÄ GPU NVIDIA (CUDA)")
        elif torch.backends.mps.is_available():
            self.device = "mps"
            print(f"üß† ƒêang t·∫£i model: {model_name} l√™n üçé Apple Silicon GPU (MPS)")
        else:
            self.device = "cpu"
            print(f"üß† ƒêang t·∫£i model: {model_name} l√™n ‚ö†Ô∏è CPU (S·∫Ω ch·∫≠m h∆°n)")
        
        # Processor: Gi√∫p chu·∫©n ho√° m√†u s·∫Øc (Normalize) theo chu·∫©n ImageNet
        try:
            self.processor = AutoImageProcessor.from_pretrained(model_name, use_fast=True)
        except:
            # Fallback n·∫øu m√°y kh√¥ng c√†i th∆∞ vi·ªán h·ªó tr·ª£ fast
            self.processor = AutoImageProcessor.from_pretrained(model_name, use_fast=False)

        # Model: M·∫°ng n∆°-ron th·ª±c hi·ªán t√≠nh to√°n
        self.model = AutoModel.from_pretrained(model_name).to(self.device)

        # Chuy·ªÉn to√†n b·ªô tr·ªçng s·ªë model sang FP16
        self.model = self.model.half() 

        # Chuy·ªÉn sang ch·∫ø ƒë·ªô 'eval' (Evaluation)
        # B√°o cho model bi·∫øt ta ƒëang d√πng ƒë·ªÉ d·ª± ƒëo√°n, kh√¥ng ph·∫£i ƒë·ªÉ train.
        # Gi√∫p kh√≥a c√°c tham s·ªë l·∫°i, ch·∫°y nhanh h∆°n v√† ·ªïn ƒë·ªãnh h∆°n.
        self.model.eval()

    def _smart_resize(self, image, max_side=1024, patch_size=14):
        """
        X·ª≠ l√Ω k√≠ch th∆∞·ªõc ·∫£nh th√¥ng minh:
        1. N·∫øu ·∫£nh qu√° to (> max_side), thu nh·ªè l·∫°i gi·ªØ t·ª∑ l·ªá.
        2. ƒê·∫£m b·∫£o k√≠ch th∆∞·ªõc cu·ªëi c√πng chia h·∫øt cho 14.
        """
        w, h = image.size
        
        # --- B∆Ø·ªöC 1: GI·ªöI H·∫†N K√çCH TH∆Ø·ªöC (QUAN TR·ªåNG) ---
        # N·∫øu c·∫°nh l·ªõn nh·∫•t v∆∞·ª£t qu√° max_side, ta ph·∫£i thu nh·ªè n√≥ l·∫°i
        # Scale v·ªÅ max_side
        if max(w, h) > max_side:
            ratio = max_side / max(w, h)
            w = int(w * ratio)
            h = int(h * ratio)
        
        # --- B∆Ø·ªöC 2: CHIA H·∫æT CHO 14 ---
        # L√†m tr√≤n xu·ªëng b·ªôi s·ªë g·∫ßn nh·∫•t c·ªßa 14
        # ƒê·∫£m b·∫£o 2 canh chia h·∫øt cho 14
        new_w = (w // patch_size) * patch_size
        new_h = (h // patch_size) * patch_size
        
        # ƒê·∫£m b·∫£o kh√¥ng b·ªã v·ªÅ 0
        new_w = max(new_w, patch_size)
        new_h = max(new_h, patch_size)
        
        # Ch·ªâ resize n·∫øu k√≠ch th∆∞·ªõc thay ƒë·ªïi
        if (new_w, new_h) != image.size:
            # print(f"   -> Resize t·ª´ {image.size} v·ªÅ {(new_w, new_h)}") # Uncomment ƒë·ªÉ debug
            return image.resize((new_w, new_h), resample=Image.Resampling.LANCZOS)
        
        return image

    def extract(self, images, input_size=1024):
        """
        H√†m ch√≠nh: Nh·∫≠n v√†o 1 ·∫£nh PIL -> Tr·∫£ v·ªÅ Vector (Numpy Array).
        """
        is_batch = isinstance(images, list) # Ki·ªÉm tra xem c√≥ ph·∫£i l√† danh s√°ch kh√¥ng
        
        # 1. X·ª≠ l√Ω Resize (H·ªó tr·ª£ c·∫£ ƒë∆°n l·∫ª v√† danh s√°ch)
        if is_batch:
            # N·∫øu l√† list, ch·∫°y smart_resize cho t·ª´ng ·∫£nh trong list
            processed_imgs = [self._smart_resize(img, max_side=input_size) for img in images]
        else:
            # N·∫øu l√† ·∫£nh ƒë∆°n, ch·∫°y b√¨nh th∆∞·ªùng
            processed_imgs = self._smart_resize(images, max_side=input_size)
        
        # 2. Preprocess (Th∆∞ vi·ªán t·ª± hi·ªÉu List ho·∫∑c Single Image)
        inputs = self.processor(
            images=processed_imgs, 
            return_tensors="pt", 
            do_resize=False, 
            do_center_crop=False
        )
        
        # inputs = {k: v.to(self.device) for k, v in inputs.items()}
        # Th·ª≠ v·ªõi FP16
        inputs = {k: v.to(self.device).half() for k, v in inputs.items()}

        # 3. Inference
        with torch.no_grad():
            outputs = self.model(**inputs)
        
        # 4. L·∫•y k·∫øt qu·∫£
        # OPTION: L·ª±a ch·ªçn kh√°c "Average Pooling"
        last_hidden_states = outputs.last_hidden_state
        embedding = last_hidden_states[:, 0, :] # L·∫•y CLS Token
        
        # 5. Tr·∫£ v·ªÅ k·∫øt qu·∫£ (X·ª≠ l√Ω chi·ªÅu d·ªØ li·ªáu)
        # .float() ƒë·ªÉ chuy·ªÉn t·ª´ FP16 v·ªÅ l·∫°i FP32 tr∆∞·ªõc khi sang CPU/Numpy
        result = embedding.float().cpu().numpy()
        
        if is_batch:
            return result # Tr·∫£ v·ªÅ m·∫£ng 2 chi·ªÅu (N, 1024)
        else:
            return result.flatten() # Tr·∫£ v·ªÅ m·∫£ng 1 chi·ªÅu (1024,)

class ImageSlicer:
    # OPTION: window_size ph·∫£i b·ªôi c·ªßa 14
    # overlap_ratio = 20%
    def __init__(self, window_size=518, overlap_ratio=0.2):
        """
        C√¥ng c·ª• c·∫Øt ·∫£nh th√†nh c√°c m·∫£nh nh·ªè (Tiles).
        Args:
            window_size: K√≠ch th∆∞·ªõc m·ªói m·∫£nh (N√™n l√† 518 cho DINOv2).
            overlap_ratio: T·ª∑ l·ªá ch·ªìng l·∫•n (0.2 = 20%).
        """
        self.window_size = window_size
        self.overlap_ratio = overlap_ratio
        
        # Stride: B∆∞·ªõc nh·∫£y c·ªßa c·ª≠a s·ªï tr∆∞·ª£t
        # N·∫øu window=518, overlap=0.2 -> stride ‚âà 414 pixel.
        # Nghƒ©a l√† c·ª© tr∆∞·ª£t 414px th√¨ c·∫Øt 1 ph√°t.
        self.stride = int(window_size * (1 - overlap_ratio))

    def _get_points(self, length):
        """
        H√†m t√≠nh to√°n c√°c ƒëi·ªÉm to·∫° ƒë·ªô c·∫Øt (d√πng chung cho c·∫£ chi·ªÅu ngang v√† d·ªçc).
        Logic: Tr∆∞·ª£t -> Tr∆∞·ª£t -> ... -> N·∫øu c√°i cu·ªëi b·ªã h·ª•t th√¨ l√πi l·∫°i cho ƒë·ªß.
        """
        if length <= self.window_size:
            return [0] # ·∫¢nh nh·ªè h∆°n c·ª≠a s·ªï th√¨ l·∫•y lu√¥n g·ªëc to·∫° ƒë·ªô 0
            
        points = []
        current = 0
        while True:
            points.append(current)
            current += self.stride
            
            # Ki·ªÉm tra n·∫øu b∆∞·ªõc ti·∫øp theo b·ªã l√≤i ra ngo√†i ·∫£nh
            if current + self.window_size >= length:
                # Th√™m ƒëi·ªÉm cu·ªëi c√πng: L·∫•y ƒë·ªô d√†i tr·ª´ ƒëi k√≠ch th∆∞·ªõc c·ª≠a s·ªï
                # ƒê·∫£m b·∫£o m·∫£nh cu·ªëi lu√¥n full size, kh√¥ng b·ªã ƒëen vi·ªÅn
                last_point = length - self.window_size
                if last_point > points[-1]: # Tr√°nh tr√πng l·∫∑p n·∫øu ·∫£nh v·ª´a kh√≠t
                    points.append(last_point)
                break
        return points

    def slice_generator(self, img):
        """
        Input: ·∫¢nh PIL (High-res).
        Output: List c√°c ·∫£nh con (PIL Images).
        """
        width, height = img.size
        
        # N·∫øu ·∫£nh nh·ªè h∆°n c·ª≠a s·ªï c·∫Øt (v√≠ d·ª• ·∫£nh 400x400 m√† c·ª≠a s·ªï 518)
        # yield ch√≠nh n√≥ r·ªìi ngh·ªâ
        if width <= self.window_size and height <= self.window_size:
            yield img
            return

        
        # T√≠nh to√°n to·∫° ƒë·ªô l∆∞·ªõi
        x_points = self._get_points(width)
        y_points = self._get_points(height)
        
        for y in y_points:
            for x in x_points:
                box = (x, y, x + self.window_size, y + self.window_size)
                # C·∫Øt v√† tr·∫£ v·ªÅ ngay l·∫≠p t·ª©c (Yield)
                yield img.crop(box)

def run_processing_pipeline(input_folder, output_json="data_vectors.json"):
    print("\nüöÄ B·∫ÆT ƒê·∫¶U QUY TR√åNH X·ª¨ L√ù D·ªÆ LI·ªÜU (PIPELINE)...")
    
    # 1. Kh·ªüi t·∫°o c√¥ng c·ª•
    # Global c·∫ßn nh√¨n t·ªïng th·ªÉ -> input_size to (1024)
    # Local c·∫ßn nh√¨n chi ti·∫øt -> input_size v·ª´a (518) kh·ªõp v·ªõi window
    # OPTION: L·ª±a chon small, base, large, giant
    extractor = FeatureExtractor(model_name='facebook/dinov2-large')

    # OPTION: chia ·∫£nh th√†nh c√°c tiles c√≥ k√≠ch th∆∞·ªõc window_size, ph·∫£i b·ªôi s·ªë c·ªßa 14
    # overlap_ratio=0.2 l√† ph√π h·ª£p
    slicer = ImageSlicer(window_size=518, overlap_ratio=0.2)
    
    database = []
    
    # L·∫•y danh s√°ch ·∫£nh
    valid_exts = ('.jpg', '.jpeg', '.png', '.webp')
    files = [f for f in os.listdir(input_folder) if f.lower().endswith(valid_exts)]
    
    # S·∫Øp x·∫øp t√™n file ƒë·ªÉ x·ª≠ l√Ω theo th·ª© t·ª± (t√πy ch·ªçn)
    files.sort()
    
    print(f"üìÇ T√¨m th·∫•y {len(files)} ·∫£nh trong {input_folder}")
    
    # V√≤ng l·∫∑p ch√≠nh (C√≥ thanh ti·∫øn tr√¨nh)
    for filename in tqdm(files, desc="üß† Extracting Features", unit="img"):
        try:
            img_path = os.path.join(input_folder, filename)
            
            # M·ªü ·∫£nh
            with Image.open(img_path) as img:
                img = img.convert('RGB')
                
                # --- A. GLOBAL FEATURE ---
                # L·∫•y vector to√†n c·∫£nh
                # OPTION: ·∫¢nh global: M√°y m·∫°nh n√™n ch·∫°y 1526
                v_global = extractor.extract(img, input_size=1526)
                # Chu·∫©n h√≥a L2 ngay 
                v_global = v_global / np.linalg.norm(v_global)
                
                # --- B. LOCAL FEATURES (Generator + Batching) ---
                tile_gen = slicer.slice_generator(img)
                
                batch_imgs = []
                local_vectors_list = []
                # OPTION: Gom [BATCH_SIZE] ·∫£nh con r·ªìi quƒÉng v√†o h√†m extract
                BATCH_SIZE = 8 # T√πy VRAM, 8 l√† an to√†n

                SAVE_INTERVAL = 10 # C·ª© xong 50 ·∫£nh th√¨ l∆∞u file 1 l·∫ßn (Tr√°nh m·∫•t ƒëi·ªán/disconnect)
                count_since_save = 0

                for tile in tile_gen:
                    batch_imgs.append(tile)
                    
                    # N·∫øu gom ƒë·ªß batch th√¨ x·ª≠ l√Ω
                    if len(batch_imgs) >= BATCH_SIZE:
                        # OPTION: N·∫øu tƒÉng input_size ·ªü ƒë√¢y -> Ph·∫£i tƒÉng ImageSlicer(window_size=518...) ·ªü tr√™n 
                        batch_vecs = extractor.extract(batch_imgs, input_size=518)
                        local_vectors_list.append(batch_vecs) # batch_vecs l√† (N, 1024)
                        batch_imgs = [] # Reset batch
                
                # X·ª≠ l√Ω batch l·∫ª cu·ªëi c√πng -> Tr∆∞·ªùng h·ª£p batch_imgs < BATCH_SIZE
                if batch_imgs:
                    batch_vecs = extractor.extract(batch_imgs, input_size=518)
                    local_vectors_list.append(batch_vecs)
                
                # --- C. FUSION (H·ª£p nh·∫•t) ---
                if local_vectors_list:
                    # N·ªëi t·∫•t c·∫£ c√°c batch l·∫°i th√†nh 1 m·∫£ng l·ªõn (Total_Tiles, 1024)
                    all_local_matrix = np.vstack(local_vectors_list)
                    
                    # T√≠nh trung b√¨nh c·ªông (Average Pooling) -> Ra (1024,)
                    v_local_mean = np.mean(all_local_matrix, axis=0)
                    v_local_mean = v_local_mean / np.linalg.norm(v_local_mean) # Chu·∫©n h√≥a
                    
                    # C√¥ng th·ª©c h·ª£p nh·∫•t: 50% Global + 50% Local
                    final_vector = (v_global * 0.5) + (v_local_mean * 0.5)
                else:
                    # Tr∆∞·ªùng h·ª£p ·∫£nh qu√° nh·ªè kh√¥ng c·∫Øt ƒë∆∞·ª£c tile n√†o
                    final_vector = v_global

                # Chu·∫©n h√≥a l·∫ßn cu·ªëi vector t·ªïng h·ª£p
                final_vector = final_vector / np.linalg.norm(final_vector)
                
                # --- D. ƒê√ìNG G√ìI ---
                # L∆∞u ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi ƒë·ªÉ Web d√πng
                # V√≠ d·ª•: filename l√† "A.jpg" -> path="thumbnail/A.webp"
                thumb_path = f"thumbnail/{os.path.splitext(filename)[0]}.webp"
                
                database.append({
                    "id": filename,
                    "highress_path": img_path,
                    "thumb_path": thumb_path,
                    "vector": final_vector.tolist() # Chuy·ªÉn numpy -> list ƒë·ªÉ l∆∞u JSON
                })
                # --- E. C∆† CH·∫æ AUTO-SAVE ---
                count_since_save += 1
                if count_since_save >= SAVE_INTERVAL:
                    # L∆∞u file t·∫°m th·ªùi
                    with open(output_json, 'w') as f:
                        json.dump(database, f)
                    count_since_save = 0
                    # print(f"   (ƒê√£ l∆∞u checkpoint: {len(database)} ·∫£nh)") # B·ªè comment n·∫øu mu·ªën xem log
                
        except Exception as e:
            tqdm.write(f"‚ùå L·ªói x·ª≠ l√Ω {filename}: {e}")

    # 4. L∆∞u file JSON
    print(f"üíæ ƒêang l∆∞u d·ªØ li·ªáu v√†o {output_json}...")
    with open(output_json, 'w') as f:
        json.dump(database, f)
    print("‚úÖ HO√ÄN T·∫§T QUY TR√åNH!")

def generate_layout(input_file="data_vectors.json", output_file="final_structure.json"):
    print(f"\nüé® ƒêANG V·∫º B·∫¢N ƒê·ªí 3D T·ª™ {input_file}...")
    
    # 1. ƒê·ªçc d·ªØ li·ªáu th√¥
    if not os.path.exists(input_file):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file {input_file}")
        return

    with open(input_file, 'r') as f:
        data = json.load(f)
    
    if not data:
        print("‚ö†Ô∏è File d·ªØ li·ªáu r·ªóng!")
        return

    # T√°ch ri√™ng danh s√°ch vector ƒë·ªÉ t√≠nh to√°n
    # data[i]['vector'] ƒëang l√† list, c·∫ßn chuy·ªÉn sang numpy array
    print("   -> ƒêang load vectors v√†o RAM...")
    vectors = np.array([item['vector'] for item in data])
    
    # 2. Ch·∫°y thu·∫≠t to√°n UMAP (Gi·∫£m t·ª´ 1024D -> 3D)
    print("   -> ƒêang ch·∫°y UMAP (Vi·ªác n√†y t·ªën ch√∫t th·ªùi gian)...")
    
    # --- C·∫§U H√åNH UMAP (QUAN TR·ªåNG) ---
    # OPTION
    reducer = umap.UMAP(
        n_components=3,    # ƒê√≠ch ƒë·∫øn l√† 3D (x, y, z)
        n_neighbors=30,    # Nh√¨n 30 h√†ng x√≥m ƒë·ªÉ ƒë·ªãnh v·ªã (S·ªë to -> C·∫•u tr√∫c to√†n c·ª•c r√µ h∆°n)
        min_dist=0.1,      # Kho·∫£ng c√°ch t·ªëi thi·ªÉu gi·ªØa c√°c ƒëi·ªÉm (S·ªë nh·ªè -> C·ª•m ch·∫∑t)
        metric='cosine',   # ƒêo g√≥c (t·ªët nh·∫•t cho vector AI)
        random_state=42    # C·ªë ƒë·ªãnh k·∫øt qu·∫£ (Ch·∫°y 10 l·∫ßn ra gi·ªëng nhau)
    )
    
    embedding_3d = reducer.fit_transform(vectors)
    
    # 3. H·∫≠u k·ª≥ to·∫° ƒë·ªô (Post-processing) - "Trang ƒëi·ªÉm"
    print("   -> ƒêang tinh ch·ªânh to·∫° ƒë·ªô (Scaling & Jitter)...")
    
    # A. Scale v·ªÅ kh√¥ng gian hi·ªÉn th·ªã (V√≠ d·ª• t·ª´ -35 ƒë·∫øn 35)
    # ƒê√¢y l√† k√≠ch th∆∞·ªõc s√¢n kh·∫•u c·ªßa b·∫°n tr√™n Web
    # OPTION
    SCENE_SIZE = 20 
    scaler = MinMaxScaler(feature_range=(-SCENE_SIZE, SCENE_SIZE))
    embedding_3d = scaler.fit_transform(embedding_3d)
    
    # B. Th√™m nhi·ªÖu (Jitter) ƒë·ªÉ t√°ch c√°c ·∫£nh tr√πng nhau
    # Tr√°nh hi·ªán t∆∞·ª£ng 2 ·∫£nh ƒë√® l√™n nhau nh·∫•p nh√°y
    noise_strength = 0.5 
    noise = np.random.uniform(-noise_strength, noise_strength, embedding_3d.shape)
    embedding_3d += noise
    
    # C. K√©o gi√£n tr·ª•c Z (Chi·ªÅu s√¢u) v√† ƒê·∫∑t l√™n m·∫∑t ƒë·∫•t
    # Nh√¢n tr·ª•c Z l√™n 1.5 l·∫ßn cho s√¢u hun h√∫t
    embedding_3d[:, 2] *= 1.5 
    
    # T·ªãnh ti·∫øn tr·ª•c Z sao cho ƒëi·ªÉm th·∫•p nh·∫•t = 0 (N·∫±m tr√™n s√†n)
    min_z = np.min(embedding_3d[:, 2])
    embedding_3d[:, 2] = embedding_3d[:, 2] - min_z

    # 4. ƒê√≥ng g√≥i k·∫øt qu·∫£ cu·ªëi c√πng
    final_data = []
    for i, item in enumerate(data):
        final_data.append({
            "id": item['id'],
            "thumb_path": item['thumb_path'], # Gi·ªØ nguy√™n ƒë∆∞·ªùng d·∫´n thumbnail
            "highress_path": item['highress_path'],
            "position": [
                round(float(embedding_3d[i, 0]), 3), # L√†m tr√≤n 3 s·ªë l·∫ª cho nh·∫π JSON
                round(float(embedding_3d[i, 1]), 3),
                round(float(embedding_3d[i, 2]), 3)
            ]
        })

    # 5. L∆∞u file
    with open(output_file, 'w') as f:
        json.dump(final_data, f)
        
    print(f"‚úÖ XONG! D·ªØ li·ªáu Web ƒë√£ s·∫µn s√†ng t·∫°i: {output_file}")
    print(f"   -> T·ªïng s·ªë ·∫£nh: {len(final_data)}")
    print(f"   -> To·∫° ƒë·ªô X, Y trong kho·∫£ng: [-{SCENE_SIZE}, {SCENE_SIZE}]")
    print(f"   -> To·∫° ƒë·ªô Z (ƒê·ªô cao): [0, {round(np.max(embedding_3d[:, 2]), 2)}]")

# main
if __name__ == "__main__":
    # Ghi l·∫°i th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu
    start_time = time.time()
    # --- B∆Ø·ªöC 1: T·∫†O THUMBNAIL ---
    # (N·∫øu ch·∫°y r·ªìi th√¨ comment l·∫°i cho nhanh)
    # create_thumbnails(INPUT_FOLDER, THUMB_FOLDER)
    
    
    # --- B∆Ø·ªöC 2: TR√çCH XU·∫§T ƒê·∫∂C TR∆ØNG (PIPELINE) ---
    # File trung gian ch·ª©a vector 1024 chi·ªÅu
    VECTOR_FILE = "data_vectors.json" 
    
    # Ki·ªÉm tra xem c√≥ c·∫ßn ch·∫°y l·∫°i b∆∞·ªõc n·∫∑ng nh·∫•t n√†y kh√¥ng
    if not os.path.exists(VECTOR_FILE):
        if os.path.exists(INPUT_FOLDER):
            run_processing_pipeline(INPUT_FOLDER, output_json=VECTOR_FILE)
        else:
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c ·∫£nh: {INPUT_FOLDER}")
    else:
        print(f"‚ÑπÔ∏è ƒê√£ t√¨m th·∫•y {VECTOR_FILE}. B·ªè qua b∆∞·ªõc tr√≠ch xu·∫•t.")

    # --- B∆Ø·ªöC 3: T·∫†O TO·∫† ƒê·ªò 3D (MAPPING) ---
    # File cu·ªëi c√πng cho Web
    # FINAL_FILE = "final_structure.json"
    
    # # B∆∞·ªõc n√†y ch·∫°y r·∫•t nhanh (v√†i gi√¢y), n√™n c·ª© ch·∫°y l·∫°i tho·∫£i m√°i
    # if os.path.exists(VECTOR_FILE):
    #     generate_layout(input_file=VECTOR_FILE, output_file=FINAL_FILE)
    # else:
    #     print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu vector ƒë·ªÉ v·∫Ω b·∫£n ƒë·ªì.")

    

    # Ghi l·∫°i th·ªùi ƒëi·ªÉm k·∫øt th√∫c
    end_time = time.time()

    # T√≠nh th·ªùi gian ch·∫°y
    elapsed_time = end_time - start_time

    print(f"Th·ªùi gian th·ª±c thi: {elapsed_time:.4f} gi√¢y")



# TODO: outputs.last_hidden_state[:, 0, :] hi·ªán t·∫°i ƒëang l·∫•y CLS TOKEN
# N√≥ l√† token ƒë∆∞·ª£c model ƒë√†o t·∫°o ƒë·ªÉ t√≥m t·∫Øt ·∫£nh

# TODO: m·ªôt "c∆° ch·∫ø b·∫£o hi·ªÉm" v√†o m√£ ngu·ªìn: L∆∞u t·ª± ƒë·ªông (Auto-save) sau m·ªói 50 ·∫£nh. -> Ch·∫°y s·ªë l∆∞·ª£ng l·ªõn
# N√™n coi l·∫°i chu·∫©n ho√° khi SCENE_SIZE = 30 th√¨ xy=SCENE_SIZE thooi, c√≤n Z ph·∫£i t√≠nh to√°n l·∫°i cho ph√π hoppwj
# b·ªï xung v√†o json hasing, n·∫øu ƒë√£ c√≥ b·ªè qua
# th√™m t√≠nh nƒÉng l·ªçc gi·ªëng b·∫±ng hasing, l·ªçc tr√πng, v√† backup + ki·ªÉm tra ƒë√£ c√≥ r·ªìi th√¨ b·ªè qua